{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "import os\n",
    "config_path = '/home/lolicon/data/dataset/Illya/model.yaml'\n",
    "root_dir = '/home/lolicon/data/dataset/Illya/'\n",
    "main_ckpt_path = '../stable-diffusion-webui/models/Stable-diffusion/CounterfeitV30_v30.safetensors'\n",
    "lora_ckpt_path = os.path.join(root_dir, 'lora_ckpt', 'epoch=1999.ckpt')\n",
    "# main_ckpt_path = os.path.join(root_dir, 'lora_ckpt', 'epoch=1999.ckpt')\n",
    "\n",
    "# weight for control net \n",
    "ctrl_pose_path = '../stable-diffusion-webui/models/ControlNet/control_sd15_openpose.pth'\n",
    "\n",
    "num_epochs = 2\n",
    "num_gpus = 1\n",
    "batch_size = 2\n",
    "logger_freq = 20000\n",
    "learning_rate = 1e-5\n",
    "sd_locked = True\n",
    "only_mid_control = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "# from annotator.canny import CannyDetector\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "\n",
    "# apply_canny = CannyDetector()\n",
    "# config_path = '/home/lolicon/data/dataset/lycoris/model.yaml'\n",
    "model = create_model(config_path).cpu()\n",
    "torch_device = 'cuda'\n",
    "# send to device\n",
    "model = model.to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.model_loader import load_state_dict\n",
    "\n",
    "# weight for unet, encoder, decoder, text embedding \n",
    "# main_ckpt_path = '../stable-diffusion-webui/models/Stable-diffusion/pastelMixStylizedAnime_pastelMixFull.safetensors'\n",
    "\n",
    "sd_ckpt = load_state_dict(main_ckpt_path)\n",
    "sd_ctrl = load_state_dict(ctrl_pose_path)\n",
    "# lora_model = load_state_dict(lora_ckpt_path)\n",
    "model.load_multi_state_dict(sd_ckpt, pose_model=sd_ctrl)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import CustomDataset\n",
    "\n",
    "\n",
    "dataset = CustomDataset(root_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Lora Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.Lora import LoRANetwork, LoRAModule\n",
    "\n",
    "lora_unet = load_state_dict(lora_ckpt_path)\n",
    "lora_network = LoRANetwork(unet=model.control_model.pose_model, lora_dim=8, alpha=4.0, weights_sd=lora_unet)\n",
    "lora_network.apply_to(apply_unet=True, apply_text_encoder=False)\n",
    "# lora_network = load_state_dict(lora_ckpt_path)\n",
    "# loar_network = Lo\n",
    "# print(model.control_model.pose_model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class Ensemble(pl.LightningModule):\n",
    "    def __init__(self, model, lora, learning_rate=1e-5, sd_locked=True, only_mid_control=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        lora.apply_to(apply_unet=True, apply_text_encoder=False)\n",
    "\n",
    "        self.model = model\n",
    "        self.lora = lora\n",
    "\n",
    "        self.model.log = self.log\n",
    "\n",
    "        self.lora.train()\n",
    "        self.num_timesteps = 40\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model.sd_locked = sd_locked\n",
    "        self.model.only_mid_control = only_mid_control\n",
    "\n",
    "    def prepare_grad_etc(self):\n",
    "        self.lora.requires_grad_(True)\n",
    "        self.model.control_model.requires_grad_(True)\n",
    "        self.model.model.requires_grad_(True)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.lora.state_dict()\n",
    "\n",
    "    def prepare_optimizer_params(self):\n",
    "        return self.lora.prepare_optimizer_params(self.learning_rate, self.learning_rate)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        params = self.prepare_optimizer_params()\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "        return opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.model.training_step(batch, batch_idx)\n",
    "\n",
    "    def on_train_batch_start(self, batch, batch_idx):\n",
    "        self.model.on_train_batch_start(batch, batch_idx)\n",
    "\n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        return self.model.on_train_batch_end(*args, **kwargs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import einops\n",
    "prompt = [\"masterpiece, best quality, seduction, cute face, 1girl, sexy, 8K, high resolution, weapon\"]\n",
    "# prompt_negative = [\"deformation, ugly, bad quality, distortion\"]\n",
    "\n",
    "# lora_network.\n",
    "# # total_model = Ensemble(model, lora_network, learning_rate, sd_locked, only_mid_control)\n",
    "# lora_network.load_state_dict(lora_unet)\n",
    "# total_model.lora.load_state_dict(lora_unet)\n",
    "# prompt = [\"1girl, aurora, blonde_hair, city_lights, cloud, cloudy_sky, diffraction_spikes, feather_hair_ornament, feathers, gradient_sky, hair_ornament, holding, horizon, illyasviel_von_einzbern, lens_flare, light_rays, long_hair, looking_at_viewer, ocean, orange_sky, outdoors, prisma_illya, red_eyes, shooting_star, sitting, sky, solo, sparkle, star_\\\\(sky\\\\), starry_sky, sun, sunlight, sunrise, sunset, twilight\"]\n",
    "prompt_negative = [\"easynegative, lowres, text, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, bad anatomy, bad hands, error, missing fingers, extra digits, fewer digits, bad feet, bad colours, missing arms, text, water print, logo\"]\n",
    "\n",
    "\n",
    "\n",
    "# using pose root_dir/hint/1.png\n",
    "hint_path = os.path.join(root_dir, 'hint', '23.png')\n",
    "source = cv2.imread(hint_path)\n",
    "source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "# require source shape to be the same as (height, width)\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "random_seed = time.time()\n",
    "\n",
    "source = cv2.resize(source, (height, width), interpolation=cv2.INTER_LINEAR)\n",
    "source = torch.tensor((source.astype(np.float32)/ 255.0)).to(torch_device).unsqueeze(0)\n",
    "source = einops.rearrange(source, 'b h w c -> b c h w').clone()\n",
    "\n",
    "print(source.shape)\n",
    "\n",
    "# import random\n",
    "# random_seed = random.randint(0, 2147483647)\n",
    "\n",
    "num_inference_steps = 40\n",
    "cfg_scale = 7.5\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prompt_parser import get_learned_conditioning, get_multicond_learned_conditioning\n",
    "import utils.prompt_parser\n",
    "from modules.sampler import VanillaStableDiffusionSampler\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from PIL import Image\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "# model.to(accelerator.device)\n",
    "# LoraNet.to(torch_device)\n",
    "# model.to(torch_device)\n",
    "\n",
    "# prompt = [\"A cool digital illustration of a steampunk computer laboratory with clockwork machines, 4k, detailed, trending in artstation, fantasy vivid colors\"]\n",
    "# prompt_negative = [\"\"]\n",
    "# orig_sampler = VanillaStableDiffusionSampler(DDIMSampler, )\n",
    "# lora_network.apply_to\n",
    "orig_sampler = DDIMSampler(model)\n",
    "\n",
    "\n",
    "# Positive Text Embedding\n",
    "positive_text_embeddings = model.get_learned_conditioning(prompt)\n",
    "print(f'the postive text embedding: {positive_text_embeddings}')\n",
    "\n",
    "# Negative Text Embedding\n",
    "negative_text_embeddings  = model.get_learned_conditioning(prompt_negative)\n",
    "print(f'the negative text embedding: {negative_text_embeddings}')\n",
    "\n",
    "# Prep latents\n",
    "torch.manual_seed(random_seed)\n",
    "latents = torch.randn((batch_size, 4, height // 8, width // 8), device='cpu')\n",
    "latents = latents.to(torch_device)\n",
    "\n",
    "add_prompt = 'illya'\n",
    "\n",
    "conditioning = {'c_crossattn': [positive_text_embeddings]}\n",
    "conditioning['pose_1'] = [source]\n",
    "unconditional_conditioning = {'c_crossattn': [negative_text_embeddings]}\n",
    "unconditional_conditioning['pose_1'] = [source]\n",
    "\n",
    "conditioning['pose_1_text'] = [model.get_learned_conditioning([f\"{add_prompt}\"])]\n",
    "unconditional_conditioning['pose_1_text'] = [model.get_learned_conditioning([f\"{add_prompt}\"])]\n",
    "\n",
    "                            #  unconditional_conditioning=negative_text_embeddings,\n",
    "                            #  conditioning=positive_text_embeddings,\n",
    "sample_ddim, intermediates = orig_sampler.sample(S=num_inference_steps, \n",
    "                             batch_size=1,\n",
    "                             shape=(4, height // 8, width // 8),\n",
    "                             conditioning=conditioning,\n",
    "                             x_T=latents,\n",
    "                             unconditional_conditioning=unconditional_conditioning,\n",
    "                             unconditional_guidance_scale=cfg_scale)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = model.decode_first_stage(sample_ddim)\n",
    "\n",
    "# Display\n",
    "image = (image / 2.0 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = model.decode_first_stage(sample_ddim)\n",
    "\n",
    "# Display\n",
    "image = (image / 2.0 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = model.decode_first_stage(sample_ddim)\n",
    "\n",
    "# Display\n",
    "image = (image / 2.0 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = model.decode_first_stage(Lora_dataset[4][0].to(accelerator.device).unsqueeze(0))\n",
    "\n",
    "# Display\n",
    "image = (image / 2.0 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoraNet.load_module()\n",
    "# LoraNet.unload_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config\n",
    "# Set the gradient\n",
    "\n",
    "class trainModel(pl.LightningModule):\n",
    "    def __init__(self, LDMmodel, LoraNet):\n",
    "        super().__init__()\n",
    "        self.model = LDMmodel\n",
    "        self.LoraNet = LoraNet\n",
    "    \n",
    "    def set_gradient(self):\n",
    "        self.model.first_stage_model = self.model.first_stage_model.to('cpu')\n",
    "        self.model.requires_grad_(requires_grad=True)\n",
    "        self.LoraNet.requires_grad_(requires_grad=True)\n",
    "    \n",
    "    def p_losses(self, x_start, t, cond, noise=None):\n",
    "        \n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        x_noisy = self.model.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        model_out = self.model.model(x_noisy, t=t, c_crossattn=[cond])\n",
    "\n",
    "        loss_dict = {}\n",
    "        if self.model.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        elif self.model.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Paramterization {self.parameterization} not yet supported\")\n",
    "\n",
    "        loss = self.model.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n",
    "\n",
    "        log_prefix = 'train' if self.model.training else 'val'\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n",
    "        loss_simple = loss.mean() * self.model.l_simple_weight\n",
    "\n",
    "        loss_vlb = (self.model.lvlb_weights[t] * loss).mean()\n",
    "        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n",
    "\n",
    "        loss = loss_simple + self.model.original_elbo_weight * loss_vlb\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss': loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def forward(self, image, cond):\n",
    "        # b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n",
    "        # assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        cond = self.model.get_learned_conditioning(cond)\n",
    "        t = torch.randint(0, self.model.num_timesteps, (image.shape[0],), device=self.model.device).long()\n",
    "        return self.p_losses(image, t, cond)\n",
    "\n",
    "\n",
    "myModel = trainModel(model, LoraNet)\n",
    "\n",
    "myModel.set_gradient()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
